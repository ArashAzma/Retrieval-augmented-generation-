{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import re\n",
    "import fitz  \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama.chat_models import ChatOllama \n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGFA():\n",
    "    \n",
    "    def __init__(self, model_name, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.all_text = self.preparePDF()\n",
    "        self.vector_db = self.prepareVectorDB()\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "        self.QUERY_PROMPT = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"به عنوان یک دستیار هوش مصنوعی، نقش شما این است که پنج نسخه جایگزین از \n",
    "            سوال داده شده را تولید کنید. این تغییرات باید عبارت‌ها و یا نحوه بیان مختلفی داشته باشند تا دیدگاه‌های \n",
    "            متنوعی را پوشش دهند و بازیابی اطلاعات مرتبط از پایگاه داده وکتور را آسان‌تر کنند، به ویژه در مواقعی که جستجوی \n",
    "            مشابهت ممکن است محدودیت‌هایی داشته باشد.\n",
    "            \n",
    "            لطفاً این سوالات جایگزین را هرکدام در یک خط جدید بنویسید.\n",
    "            \n",
    "            سوال اصلی: {question}\"\"\",\n",
    "        )\n",
    "        self.retriever = MultiQueryRetriever.from_llm(self.vector_db.as_retriever(), self.llm, prompt=self.QUERY_PROMPT)\n",
    "        self.template = \"\"\"شما یک استاد دانشگاه با تخصص در تاریخ ایران و ادبیات ایران و بخصوص شاهنامه هستید و مدرکی در این زمینه دارید. \n",
    "                    شما سوال زیر را برای دانشجویانتان تعیین کرده‌اید و حالا دارید یک پاسخ واضح و آسان‌فهم برای آن ارائه می‌دهید.\n",
    "                    لطفاً پاسخ سوال را تنها بر اساس متن زیر از کتاب *شاهنامه* بنویسید.\n",
    "                    \n",
    "                    لطفاً سعی کنید پاسخ را مانند یک حرفه‌ای ارائه دهید و در صورت امکان از مثال‌ها استفاده کنید.\n",
    "                    به هیچ عنوان از کلمات انگلیسی استفاده نکن\n",
    "                    زمینه: {context}\n",
    "                    \n",
    "                    سوال: {question}\n",
    "                    \"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.template)\n",
    "        self.chain = (\n",
    "            {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def prepareVectorDB(self):\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='HooshvareLab/bert-fa-zwnj-base')\n",
    "        chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_text(self.all_text)\n",
    "        documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "        vector_db = Chroma.from_documents(documents, embeddings)\n",
    "        return vector_db\n",
    "        \n",
    "    def preparePDF(self):\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        all_text = \"\"\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            text = text.replace('\\n', ' ')\n",
    "            all_text += self.preprocess(text)\n",
    "            \n",
    "        return all_text\n",
    "    \n",
    "    def invoke(self, query):\n",
    "        return self.chain.invoke(query)\n",
    "        \n",
    "    def preprocess(self, text, stopwords=None, normalizer=False, lemmatizer=False, stemmer=False):\n",
    "        normalizer = Normalizer(correct_spacing=True, remove_diacritics=True, remove_specials_chars=True, unicodes_replacement=True)\n",
    "        lemmatizer = Lemmatizer()\n",
    "        stemmer = Stemmer()\n",
    "\n",
    "        def remove_stopwords(text, stopwords):\n",
    "            text=str(text)\n",
    "            filtered_tokens = [token for token in text.split() if token not in stopwords]\n",
    "            filtered_text = ' '.join(filtered_tokens)\n",
    "            return filtered_text\n",
    "\n",
    "        def remove_emoji(text): \n",
    "            emoji_pattern = re.compile(\"[\"\n",
    "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                        u\"\\U00002702-\\U000027B0\"\n",
    "                        u\"\\U000024C2-\\U0001F251\"\n",
    "                        u\"\\U0001f926-\\U0001f937\"\n",
    "                        u'\\U00010000-\\U0010ffff'\n",
    "                        u\"\\u200d\"\n",
    "                        u\"\\u200c\"\n",
    "                        u\"\\u2640-\\u2642\"\n",
    "                        u\"\\u2600-\\u2B55\"\n",
    "                        u\"\\u23cf\"\n",
    "                        u\"\\u23e9\"\n",
    "                        u\"\\u231a\"\n",
    "                        u\"\\u3030\"\n",
    "                        u\"\\ufe0f\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "            \n",
    "            return emoji_pattern.sub(r' ', text)\n",
    "\n",
    "        def remove_halfspace(text): \n",
    "            emoji_pattern = re.compile(\"[\"                \n",
    "                        u\"\\u200c\"              \n",
    "            \"]+\", flags=re.UNICODE)\n",
    "            \n",
    "            return emoji_pattern.sub(r' ', text) \n",
    "\n",
    "        def remove_link(text): \n",
    "            return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', str(text))\n",
    "\n",
    "        def remove_picUrl(text):\n",
    "            return re.sub(r'pic.twitter.com/[\\w]*',\"\", str(text))\n",
    "\n",
    "        def remove_rt(text):\n",
    "            z = lambda text: re.compile('\\#').sub('', re.compile('RT @').sub('@', str(text), count=1).strip())\n",
    "            return z(text)\n",
    "\n",
    "        def remove_hashtag(text):\n",
    "            return re.sub(r\"#[^\\s]+\", '', str(text))\n",
    "\n",
    "        def remove_mention(text):\n",
    "            return re.sub(r\"@[^\\s]+\", '', str(text))\n",
    "\n",
    "        def remove_email(text): \n",
    "            return re.sub(r'\\S+@\\S+', '', str(text))\n",
    "\n",
    "        def remove_numbers(text): \n",
    "            return re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', ' ', str(text))\n",
    "\n",
    "        def remove_html(text):\n",
    "            html_pattern = re.compile('<.*?>')\n",
    "            return html_pattern.sub(r'', str(text))\n",
    "\n",
    "        def remove_quote(text): \n",
    "            return  str(text).replace(\"'\",\"\")\n",
    "\n",
    "        def remove_chars(text): \n",
    "            return  re.sub(r'[$+&+;+]|[><!+،:’,\\(\\).+]|[-+]|[…]|[\\[\\]»«//]|[\\\\]|[#+]|[_+]|[—+]|[*+]|[؟+]|[?+]|[\"\"]', ' ', str(text))\n",
    "\n",
    "        def remove_englishword(text): \n",
    "            return re.sub(r'[A-Za-z]+', '', str(text))\n",
    "\n",
    "        def remove_extraspaces(text):\n",
    "            return re.sub(r' +', ' ', text)\n",
    "\n",
    "        def remove_extranewlines(text):\n",
    "            return re.sub(r'\\n\\n+', '\\n\\n', text)\n",
    "\n",
    "        def lemmatizer_text(text):\n",
    "            words = []\n",
    "            for word in text.split():\n",
    "                words.append(lemmatizer.lemmatize(word))\n",
    "            return ' '.join(words)\n",
    "\n",
    "        def stemmer_text(text):\n",
    "            words = []\n",
    "            for word in text.split():\n",
    "                words.append(stemmer.stem(word))\n",
    "            return ' '.join(words)\n",
    "\n",
    "        def normalizer_text(text, normalize, lemmatize, stemm):\n",
    "            if normalize:\n",
    "                text = normalizer.normalize(text)\n",
    "            if stemm:\n",
    "                text = stemmer_text(text)\n",
    "            if lemmatize:\n",
    "                text = lemmatizer_text(text)\n",
    "            return text\n",
    "\n",
    "        def preprocess_text(text, stopwords=None, normalizer=False, lemmatizer=False, stemmer=False):\n",
    "            text = remove_link(text)\n",
    "            text = remove_picUrl(text)\n",
    "            text = remove_englishword(text)\n",
    "            if stopwords:\n",
    "                text = remove_stopwords(text, stopwords)\n",
    "            text = remove_emoji(text)\n",
    "            text = remove_rt(text)\n",
    "            text = remove_mention(text)\n",
    "            text = remove_emoji(text)\n",
    "            text = remove_hashtag(text)   \n",
    "            text = remove_email(text) \n",
    "            text = remove_html(text) \n",
    "            text = remove_chars(text)\n",
    "            text = remove_numbers(text)\n",
    "            text = remove_quote(text)\n",
    "            text = remove_extraspaces(text)\n",
    "            text = remove_extranewlines(text)\n",
    "            text = remove_halfspace(text) \n",
    "            text = normalizer_text(text, normalizer, lemmatizer, stemmer)\n",
    "            return text\n",
    "        \n",
    "        return preprocess_text(text, stopwords=None, normalizer=False, lemmatizer=False, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name HooshvareLab/bert-fa-zwnj-base. Creating a new one with mean pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pdf_path='arash.pdf'\n",
    "# model_name = \"llama3.2:1b\"\n",
    "model_name = 'partai/dorna-llama3:8b-instruct-q4_0'\n",
    "\n",
    "rag = RAGFA(model_name, pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در جواب، متنی که خلاصه شده است. \n",
      "\n",
      "در ابتدا، داستان آرش کمانگیر شجاع و برترین فرماندهان نظامی ایرانه در تاریخ شناخته شده است. او تیر خود را برای برجسته کردن افتخار خود از مرزهای دور دست برای هدف برد و طوالنی ترین راهی را که هیچکس قبل از او طی نکرده بود، انتخاب کرد و به عنوان یکی از اسطوره های باشکوه تاریخ شناخته شد.\n",
      "\n",
      "سپس داستان آرش کمانگیر با دیدن زیبایی و خوبی زنان تورانی تصمیم گرفت که با آن ها مراقبت کند و از آن ها کمک کند تا سازگار شوند و نشانگر انسانیت و مدارا در برخورد با بردگان بود. \n",
      "\n",
      "در پايان، داستانی که شامل ماجراجویی پراز افتخار و شجاعت است و در برابر دشمن او را به یک قهرمان تبدیل کرد.\n"
     ]
    }
   ],
   "source": [
    "query = ''\n",
    "# query = 'دلیل اینکه داستان آرش در شاهنامه نیست چیه؟'\n",
    "# query = 'چرا آرش در شاهنامه فردوسی نیست؟'\n",
    "# query = 'آرش کی بود؟'\n",
    "query = 'متن رو خلاصه کن'\n",
    "\n",
    "\n",
    "text = rag.invoke(query)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'آرش که بود؟', \n",
    "    'کمان آرش از چی ساخته شده بود؟'\n",
    "]\n",
    "\n",
    "with open('output.txt', 'a', encoding='utf-8') as f:\n",
    "    for query in queries:\n",
    "            output = rag.invoke(query) \n",
    "            f.write(f'\\nجواب:\\n {output}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
